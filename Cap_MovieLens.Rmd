---
title: "Capstone Project1 : MovieLens"
author: "Emmanuel Antiri"
date: '2022-06-25'
output: pdf_document
---

# 1. Introduction

Feedback is an important component in building and sustaining organizations for the foreseeable future. For instance, valuable feedback could assist businesses in improving their processes, products, and strategies. In recent times, movie ratings have taken prominence in literature and in practice, inspired by the Netflix challenge. In this report, we build a recommendation system that predicts movie ratings using data obtained from the MovieLens dataset. The final model evaluated on the validation set achieved an RMSE of **0.864528** (lower than the threshold of 0.86490). 

## 1.1 Description of Dataset 

The MovieLens dataset was used to build the recommendation system. Using random sampling, the dataset was first split into validation set (10% of the MovieLens dataset) and edx set (the remaining 90% of the MovieLens dataset). The edx set was further split into train and test sets. The train set was 90% of the edx dataset whiles the test set was 10%. It should be noted that after each of the two splits, reasonable joins are done to ensure that same users and movies are present in both sets. The dataset is made up of six (6) variables: userId, movieId, rating, timestamp, title and genres. UserId and MovieId variables specify unique identification of users and movies respectively. The rating variable specifies the rating score given to a movie by a user. Timestamp specifies the specific time that the rating was given by the user. The title variable specifies the title and release year of the movie. Genre indicates the type of content of the movie. 

## 1.2 Goal

The main goal of the project is to propose a recommendation system that achieves an RMSE below 0.86490. In doing so, the paper also demonstrate that the proposed algorithm is easily interpretative and computationally inexpensive. Moreover, the paper would showcase how individual, movie and time effects influence movie ratings. 

## 1.3 Key Steps

The steps used in building the final recommendation system is outlined below: 
1. The overall mean of ratings is used as base prediction and rating variations about the mean is recognized as the error term. 
2. Movie effects is incorporated into the prior step. 
3. Individual user effects is incorporated into the prior step. 
4. Ascertaining whether regularization produces superior results. 
5. Incorporating time effects (year of movie release and year of movie rating) into Step 3 because regularization did not produce much difference.
6. Incorporating the interaction term of year of movie release and year of movie rating into the prior step. 

It should be noted at each step of the process, the performance of the model in terms of RMSE was evaluated using the test set. Finally, the final algorithm was evaluated on the validation set which achieved an RMSE lower than the threshold. The R software was used in performing the analysis.



```{r message=FALSE, warning=FALSE, include=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#title = as.character(title),
#genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#############################
# Generate Train and Test Set
#############################

# generate reproducible test set partition
set.seed(1, sample.kind="Rounding") # Set random seed to produce reproducible outcome `
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE) # Create random index for partitioning (10% and 90%)
train <- edx[-test_index,] # Obtain train set using the index created. 
temp <- edx[test_index,] # Obtain test set using the index created. 

# Make sure userId and movieId in test set are also in train set to preserve necessary columns
test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from test set back into train set to prevent leakage
removed <- anti_join(temp, test)
train <- rbind(train, removed)

# remove redundant objects to save space
rm(removed, temp, test_index) 
```

# 2. Methods/Analysis

## 2.1 Data Cleaning

The train dataset was used to build the models that were evaluated on the test data. During splitting of data, reasonable joins were done to ensure that same users and movies are present in both sets. The dataset is made up of six (6) variables: userId, movieId, rating, timestamp, title and genres. Derived variables used in estimation included year of movie release, year of movie rating and interaction term of year of movie release and year of movie rating. 

```{r message=FALSE, warning=FALSE, include=FALSE}
# Examine the train data to understand characteristics
train %>% as_tibble()


# Obtain unique summary statistics from the train dataset
summary_train <- train %>% summarise(n_users=n_distinct(userId),# unique users from train
                                     n_movies=n_distinct(movieId),# unique movies from train
                                     min_rating=min(rating),  # the lowest rating 
                                     max_rating=max(rating) # the highest rating 
)
summary_train
```

## 2.2 Data Exploration and Visualization

### 2.2.1 Movie Ratings

From the train set, we realize that `r summary_train$n_users` unique users rated `r summary_train$n_movies` unique movies, with rating ranging from `r summary_train$min_rating` to `r summary_train$max_rating`. Figure 1 below reviews some characteristics of the distribution of ratings. First, Figure 1 shows that users are more likely to give favourable ratings; the average rating is higher than the mid-point rating of 2.75 stars. The modal rating was 4 stars. Additionally, the lowest star rating of 0.5 had the least number of user ratings. 

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Understanding Ratings : Plot the Distribution of Ratings in the Train Set
train %>% 
  ggplot(aes(rating)) +
  geom_bar() +
  theme_light() +
  ggtitle("Figure 1. Distribution of Rating") +
  theme(plot.title = element_text(face = "bold")) +
  xlab("Rating") +
  ylab("Count")
```

### 2.2.2 Movie Ratings of Users

From the train set, we note that the number of users does not equal the number of movie reviews.  Therefore, not all users watched/rated all movies, and possibly not all movies were rated by the same users. Figure 2 below shows evidence that not all movies were rated by the same users and not all users rated every movie. The few coloured indications on the plot show that users rated unequal number of movies. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
## Only the plot is needed for further analysis
## Ascertain how the top 5 frequent movieIds look like. 
keep <- train %>% 
  count(movieId) %>% 
  top_n(5, n) %>% 
  .$movieId

# Understanding what the top users say about the movies. 
tab <- train %>% 
  filter(movieId%in%keep) %>% 
  filter(userId %in% c(13:20)) %>% 
  select(userId, title, rating) %>% 
  mutate(title = str_remove(title, ", The"),
         title = str_remove(title, ":.*")) %>%
  spread(title, rating)
#tab %>% knitr::kable()


#Examining the recommendation challenge: Not every user watched/rated every movie. 
# matrix for a random sample of 100 movies and 100 users with yellow 
# indicating a user/movie combination for which we have a rating.
users <- sample(unique(train$userId), 100)  # Sample 100 users from the dataset
rafalib::mypar()
train %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users", 
        main = 'Figure 2. Plot of Rating by 100 Sample Users') #Plotting the Movie Rating by Users
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

### 2.2.3 Distribution of Users and Movie Ratings

The dissimilar pairing of the frequency of users as against number of movies rated is also represented in Figure 3 and Figure 4 below. In both plots, the number of similar group of users or movies is logarithmic transformed to base 10. 

Figure 3 below represents the distribution of movies from the train set, showing an approximately mesokurtic distribution. The plot shows that most of the rated movies are within the middle-tier counts of movies, though there exist movies that are very popular or very rare. Moreover, Figure 3 shows that there are more rare movies than there are popular movies, though there are intervals within the lower band where no movies were rated. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plotting the Distribution of movies: using ggplot (in 50 bins), with log10 scaling on x-axis
train %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 50, color = "blue") + 
  scale_x_log10() + 
  theme_light() +
  ggtitle("Figure 3. Distribution of Movies") +
  theme(plot.title = element_text(face = "bold"))
```


Figure 4 below represents the distribution of users from the train set, showing right-skewed distribution. The plot shows that most users rated few movies whiles few users rated many movies. In other words, the modal and average distributed value of usersâ€™ engagement with movie rating is below the median value. The skewness of the distribution means that the median value will provide a better center of dispersion than the average value.

Overall, Figure 3 and Figure 4 provide some cues in subsequently modelling the prediction of ratings. First, the plots show that there exist different characteristics of users and movies and these effects ought to be examined further and incorporated into models that tend to predict movie ratings. Also, the evidence that tends to show that there exist unequal number of users and movie rating pairing are shown by combining both plots. Obviously, these distributions are more likely to exist in real life. It is very intuitive to realize that movie watching, and to a lesser degree, movie rating is skewed because not all movies become popular. In the next section, we would further examine the proposition that not all movies are popular by understanding the distributed rating of movies by genres. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plotting the Distribution of Users; using ggplot (in 50 bins), with log10 scaling of x-axis
train %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 50, color = "orange") + 
  scale_x_log10() +
  theme_light() +
  ggtitle("Figure 4. Distribution of Users") +
  theme(plot.title = element_text(face = "bold"))
```

### 2.2.4 Movie Ratings by Genre

```{r message=FALSE, warning=FALSE, include=FALSE}
###Rating of Movies by Genre
# remove redundant objects to save space
rm(users, tab, keep)

###Rating of Movies by Genre
#Obtain Summary of Ratings by Genres (Sum and Frequency of Ratings)
ratings_genres <- train %>% group_by(genres) %>% 
  mutate(Sum = sum(rating), Frequency = n()) %>% 
  select("genres", "Frequency", "Sum") %>%
  distinct()
```
In basic terms, the genre of a movie reflects the type of content to expect of the movie. Each movie can be categorized in at least one movie genre. There were `r nrow(ratings_genres)` different combination of genres for movies contained in the train set. Table 1 and Table 2 provide the summary of the top ten (10) and the last ten (10) movies arranged by average ratings. The tables also reveal different frequency of ratings per movie. Thus, this difference could connote that movie with few reviews could have upward or downward biases in rating. The effects of the differing number of ratings would be examined in the modelling phase. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Obtain Average ratings to 4 decimal places
ratings_genres <- as.data.frame(ratings_genres) #Convert tibble to dataframe to easily obtain more than 2 decimals in manipulation
ratings_genres$Average_rating <- round((ratings_genres$Sum/ratings_genres$Frequency), 4) #Obtain Average ratings to 4 decimal places
ratings_genres$Sum <- NULL  #Remove redundant Sum column
ratings_genres <- arrange(ratings_genres, desc(Average_rating)) # Arrange dataframe in descending order of Average Ratings
head(ratings_genres, 10) #Showcase first 10 genres by Average rating
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
tail(ratings_genres, 10) #Showcase last 10 genres by Average rating
```

### 2.2.5 Movie Ratings: Time Effects
```{r message=FALSE, warning=FALSE, include=FALSE}
################################################################
# Exploratory Analysis : Understanding Time Effects of Ratings
################################################################
## Install and load necessary library for manipulating time effect
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(lubridate)

#Time Effects: Year Movie was rated & Year Movie was released
train$year_rated <- year(as_datetime(train$timestamp))   # Create year movie was rated in the train set
train$year_released <- gsub(".*[(]|[)].*", "", train$title)   # Create year movie was released in the train set
```

In this section, we examine the time effects on ratings. Ratings has not been the same across year of rating (Figure 5) and year of movie release(Figure 6). In all cases, the average rating was above 3.20, an indication that individuals then to rate favorably than not. However, we note marked differences in trend based on movie released. First, variations in ratings of old movies tend to be higher than the current movies. The plot shown an inverted shape trend of movie released. Movies released between 1927 and 1978 were rated consistently above 3.65 on average. However, from the year 1979, the average rating of movies shown a downward trajectory. 

```{r message=FALSE, warning=FALSE, include=FALSE}
## Year of Movie Ratings
#Obtain Summary of Ratings by Year of Movie Ratings (Sum and Frequency of Ratings)
ratings_yrated <- train %>% group_by(year_rated) %>% 
  mutate(Sum = sum(rating), Frequency = n()) %>% 
  select("year_rated", "Frequency", "Sum") %>%
  distinct()

#Obtain Average ratings to 4 decimal places
ratings_yrated <- as.data.frame(ratings_yrated) #Convert tibble to dataframe to easily obtain more than 2 decimals in manipulation
ratings_yrated$Average_rating <- round((ratings_yrated$Sum/ratings_yrated$Frequency), 4) #Obtain Average ratings to 4 decimal places
ratings_yrated$Sum <- NULL  #Remove redundant Sum column
ratings_yrated <- arrange(ratings_yrated, year_rated) # Arrange dataframe year chronologically
head(ratings_yrated, 10)  #Showcase first 10 years

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Plot the Evolution of Average Rating per year Rated (year 1995 is excluded bcos of single observation(not representative))
ratings_yrated[2:15,] %>% ggplot(aes(x = year_rated, y = Average_rating, label = round(Average_rating, 3))) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("Year Rated") +
  ylab("Average Rating") +
  ggtitle("Figure 5. Evolution of Average Ratings by Year Rated") +
  theme(plot.title = element_text(face = "bold")) +
  geom_text(nudge_y = -0.005, color = 'blue') 
```


```{r message=FALSE, warning=FALSE, include=FALSE}
##Year of Movie Release
#Obtain Summary of Ratings by Year of Movie Release (Sum and Frequency of Ratings)
ratings_yreleased <- train %>% group_by(year_released) %>% 
  mutate(Sum = sum(rating), Frequency = n()) %>% 
  select("year_released", "Frequency", "Sum") %>%
  distinct()

#Obtain Average ratings to 4 decimal places
ratings_yreleased <- as.data.frame(ratings_yreleased) #Convert tibble to dataframe to easily obtain more than 2 decimals in manipulation
ratings_yreleased$Average_rating <- round((ratings_yreleased$Sum/ratings_yreleased$Frequency), 4) #Obtain Average ratings to 4 decimal places
ratings_yreleased$Sum <- NULL  #Remove redundant Sum column
ratings_yreleased <- arrange(ratings_yreleased, year_released) # Arrange dataframe in descending order of Average Ratings
head(ratings_yreleased, 10)  #Showcase first 10 years
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Plot the Evolution of Average Rating per year Rated using ggplot
ratings_yreleased %>% ggplot(aes(x = year_released, y = Average_rating, group = 1)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("Year Released") +
  ylab("Average Rating") +
  ggtitle("Figure 6. Evolution of Average Rating by Year of Movie Release") +
  theme(plot.title = element_text(face = "bold")) +
  theme(axis.text.x = element_text(angle = 90, size = 6, vjust = 0.5, hjust=1)) 
```


## 2.3 Modeling Approach

In this section of the report, we examine the techniques used in modelling. The accuracy of the recommendation system would be evaluated using the Root Mean Squared Error (RMSE). In simple terms, the RMSE is the standard deviation of the prediction errors. In geometric terms, it measures how concentrated or far the error terms are from the line of best fit. There are many advantages associated with using the RMSE in evaluating recommendation system. First, it is easily understood since it is in the same unit as the original dataset. Next, it is a mathematically convenient method in evaluating distance and gradient metrics, making it preferred to alternative evaluation metrics such as the Mean Average Error (MAE). Additionally, the RMSE tend to penalize large errors, thus could easily help us detect the consequences of outlier predictions in our methods. The RMSE is given by the formula below: 
$$\mbox{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2}$$
where $Y_i$ is the actual $i$-th movie rating, $\hat{Y}_i$ is the predicted rating and N is the number of observations. The best model should have the lowest RMSE among models with RMSE < 0.86490. The RMSE metric was represented in coding by the following:

```{r echo=TRUE, message=FALSE, warning=FALSE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


### 2.3.1 Method 1: The Overall Mean

The first method to be examined is the overall average rating of the movie by users irrespective of any other characteristics. In this model, we assume that ratings of all users tend to a particular average and that differences in individual ratings represent random variations. Consequently, the model assumes that the expected value of ratings equals the overall mean rating. The overall mean model can be represented in the formula below: 
$$Y_{u,i} = \mu + \varepsilon_{u,i}$$

```{r message=FALSE, warning=FALSE, include=FALSE}
## Model 1: Overall Mean
mu <- mean(train$rating) # compute average ratings for all observations (irrespective of any characteristics)
mu

rmse_model1 <- RMSE(test$rating, mu) # compute root mean standard error
rmse_model1
```

where $Y_{u,i}$ is the $i$-th movie rating of user $u$, $\mu$ is the overall mean and $\varepsilon_{u,i}$ is the error term. 

From the estimation, the overall mean rating of the train set is `r mu` and the corresponding RMSE on the test data is `r round(rmse_model1, 6)`.

### 2.3.2 Method 2: Incorporating Movie Effects

The perceived quality of movies and their associated ratings differ from movie to movie. Therefore, users would rate movies differently; an indication that there are movie effects in rating. Hence, the prior method that did not consider movie effects is likely biased. Incorporating the movie effects can be represented in the model below: 

$$Y_{u,i} = \mu +b_i + \varepsilon_{u,i}$$
where $Y_{u,i}$ is the $i$-th movie rating of user $u$, $\mu$ is the overall mean, $\varepsilon_{u,i}$ is the error term and $b_i$ is the movie effect. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
## Model 2: Incorporate Movie Effects
# fit <- lm(rating ~ as.factor(movieId), data = train) :- this will crash R: lm estimation will be computationally expensive

###Computing the RMSE After incorporating movie effect
mu <- mean(train$rating) #estimate the mean of ratings
# obtain below tibble for the movie effect.
movie_avgs <- train %>% 
  group_by(movieId) %>% 
  summarise(b_i = mean(rating - mu))

## Plot the Movie Effect
movie_avgs %>% 
  ggplot(aes(b_i)) + 
  geom_histogram(bins = 50, color = "black") + 
  theme_light() +
  ggtitle("Figure 7. Movie Effect") +
  theme(plot.title = element_text(face = "bold"))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Compute predicted ratings by incorporating movie effect
predicted_ratings <- mu + test %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

# Obtain RMSE for Model 2: Incorporating Movie Effect
rmse_model2 <- RMSE(test$rating, predicted_ratings)
rmse_model2
```

The approximated estimated residual for this model is given as 
$$\varepsilon_{i,u}= y_{u,i} - \hat{\mu}$$
The RMSE of the model with movie effects incorporated is `r round(rmse_model2, 6)`. 

### 2.3.3 Method 3: Incorporating User Effects

Not surprisingly, the ratings of users tend to differ from user to user. In other words, there are differences in user ratings owing to several inherent characteristics. As regard to any form of utility, individual motivations for ratings are varied and based on multiple factors. For instance, some users are very critical whiles others are liberal. Also, some users are technical in their approach to movie ratings whiles others rate movies from non-technical lenses. Irrespective of reason for specific ratings, users tend to be unique in their ratings. The uniqueness of user ratings is incorporated in the previous model and represented below: 
$$Y_{u,i}= \mu + b_i + b_u + \varepsilon_{u,i}$$
where $Y_{u,i}$ is the $i$-th movie rating of user $u$, $\mu$ is the overall mean, $b_i$ is the movie effect, $b_u$ is the user effect and $\varepsilon_{u,i}$ is the error term. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Model 3: Incorporate User Effects
# Plot individual User Effects Incorporated into prior model
train %>% 
  group_by(userId) %>% 
  summarise(b_u = mean(rating - mu)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 50, color = "black") + 
  theme_light() +
  ggtitle("Figure 8. Individual User Effects") +
  theme(plot.title = element_text(face = "bold"))
```


```{r message=FALSE, warning=FALSE, include=FALSE}
##lm(rating ~ as.factor(movieId) + as.factor(userId)) #lm will crash R

# compute user effect b_u
user_avgs <- train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))

# compute predicted values on test set
predicted_ratings <- test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

# Obtain RMSE for Model 3: Individual User Effects
rmse_model3 <- RMSE(test$rating, predicted_ratings)
rmse_model3

```

The approximated estimated residual for this model is given as 
$$\varepsilon_{u,i}=y_{u,i} - \hat{\mu} - \hat{b}_i$$
The RMSE of the model with movie and user effects incorporated is `r round(rmse_model3, 6)`. 

### 2.3.4 Method 4: Regularization

Thus far, we have noted that the number of user ratings per movie is not same, some movies are rated more than others. Therefore, there is the tendency for rare movies with few good or bad reviews to have very high or bad average reviews respectively, especially when rated by critical or liberal reviewers. Additionally, popular movies with many reviews would generally tend to have mid-level ratings because there is the high likelihood of such movies receiving ratings from both non-technical and technical audiences. 
The method of regularization incorporates the number of reviewers per movie into the movie rating predicting, penalizing the ratings of movies with few ratings. A basic assumption herein is that large errors do increase RMSE and we would accept to be conservative when unsure about true ratings. One important component of the model with regularization is to choose an appropriate penalty term ($\lambda$) through minimizing the below: 


$$\frac{1}{N}\sum_{u,i}(y_{u,i} - \mu - b_i - b_u)^2 + \lambda(\sum_{i}b_{i}^2 + \sum_{u}b_{u}^2)$$

```{r message=FALSE, warning=FALSE, include=FALSE}
## Model 4: Regularization
# Examining whether the frequency of user ratings affect prediction. 

# Examining relationship btn movies and titles
movie_titles <- train %>% 
  select(movieId, title) %>%
  distinct()

# use cross-validation to pick a lambda:
lambda <- seq(0, 10, 0.25)

rmses <- sapply(lambda, function(l){
  mu <- mean(train$rating)
  
  b_i <- train %>% 
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    train %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(train$rating, predicted_ratings))
})

#Form dataframe of lambda values and rmses for plotting
lambda_rmse_data <- data.frame(lambda, rmses)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Plot RMSE against lambda 
lambda_rmse_data %>% ggplot(aes(lambda, rmses)) +
  geom_point() +
  theme_light() +
  ggtitle("Figure 9. Regularization Method: Plot of RMSES and Lambda") +
  theme(plot.title = element_text(face = "bold"))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# pick lambda with minimun rmse
min_lambda <- lambda[which.min(rmses)]
# print optimal lambda for regularization 
min_lambda
```
Using cross-validation, the plot above shows the various combinations of lambdas and their corresponding RMSEs. The optimal penalty term is `r min_lambda`. 

```{r message=FALSE, warning=FALSE, include=FALSE}
# compute movie effect with regularization on train set
b_i <- train %>% 
  group_by(movieId) %>%
  summarise(b_i = sum(rating - mu)/(n()+min_lambda))

# compute user effect with regularization on train set
b_u <- train %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - b_i - mu)/(n()+min_lambda))

# # compute predicted values on test set 
predicted_ratings <- 
  test %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

# Obtain RMSE on Regularization Model
rmse_model4 <- RMSE(test$rating, predicted_ratings)
rmse_model4

#There is not much difference using Regularization. 
```

The RMSE on movie regularization and user effects result in an RMSE of `r round(rmse_model4, 6)` on the test set, not much improvement from the previous method. The result show that the few ratings for some movies did not impact on the predicted ratings that much in this data. Therefore, the latter method would exclude regularization to reduce further complexity. 

### 2.3.5 Method 5: Modelling Evidence of Time Effects


At this stage, the movie year time effects were incorporated into the model to understand how the year of movie release and year of movie rating affected actual rating. Prior, the plot of movie release and movie rating had shown variation across time periods. The contributions of year of movie release and year of movie ratings (without regularization) changes the model to the following: 

$$Y_{u,i}= \mu + b_i + b_u + b_{yreleased} + b_{yrated} + \varepsilon_{u,i}$$

where $Y_{u,i}$ is the $i$-th movie rating of user $u$, $\mu$ is the overall mean, $b_i$ is the movie effect, $b_u$ is the user effect, $b_{yreleased}$ is year of movie release, $b_{yrated}$ is year of movie rated and $\varepsilon_{u,i}$ is the error term. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
#Time Effects: Year Movie was rated & Year Movie was released
#Create year rated and year released columns in test set (Already created in train set)
test$year_rated <- year(as_datetime(test$timestamp))   # Obtain year movie was rated in the test set
test$year_released <- gsub(".*[(]|[)].*", "", test$title)   # Obtain year movie was released in the test set


#Incorporate year movie was released
yreleased_effect <- train %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = "userId") %>% 
  group_by(year_released) %>%
  summarise(b_yreleased = mean(rating - mu - b_i - b_u))

#Plot the year movie was released
train %>% 
  left_join(yreleased_effect, by = 'year_released') %>%
  group_by(year_released) %>% 
  ggplot(aes(b_yreleased)) + 
  geom_histogram(bins = 100, color = "black") + 
  theme_light() +
  ggtitle("Figure 10. Incorporating Year Movie was Released") +
  theme(plot.title = element_text(face = "bold"))

```

Figure 10 and Figure 11 shows the distribution of subsequently incorporating the year movie was released and the year movie was rated in the model. Both plots show that most observations are centered around zero (0), though there exist skewness. Figure 11 exhibits much skewness to the positive, an indication that there is tendency for ratings when accounted for other properties such as movie and user effects to be at least at the mid-scale. This evidence of the data throw more light on Figure 1, which shown that individuals are likely to rate movies favorably. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Incorporate year movie was rated
yrated_effect <- train %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = "userId") %>%
  left_join(yreleased_effect, by = 'year_released') %>%
  group_by(year_rated) %>%
  summarise(b_yrated = mean(rating - mu - b_i - b_u - b_yreleased))


#Plot the year movie was rated
train %>% 
  left_join(yrated_effect, by = 'year_rated') %>%
  group_by(year_rated) %>% 
  ggplot(aes(b_yrated)) + 
  geom_histogram(bins = 100, color = "black") + 
  theme_light() +
  ggtitle("Figure 11. Incorporating Year Movie was Rated") +
  theme(plot.title = element_text(face = "bold"))

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# compute predicted values on test set
predicted_ratings <- test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(yreleased_effect, by='year_released') %>%
  left_join(yrated_effect, by = 'year_rated') %>%
  mutate(pred = mu + b_i + b_u + b_yreleased + b_yrated) %>%
  .$pred

# Obtain RMSE for model 
rmse_model5 <- RMSE(test$rating, predicted_ratings)
rmse_model5
```

The RMSE of the resulting model at this stage on the test set is `r round(rmse_model5, 6)`. 

### 2.3.6 Method 6: Time Interaction Effect

In here, we examine the improvement of the model by incorporating an interaction term effect between the year of movie released and the year of movie rated. Specifically, the model proposes that movie rating can differ based on the combination of year release and year of rating. The contribution of the interaction effect changes the model to the following: 

$$Y_{u,i}= \mu + b_i + b_u + b_{yreleased} + b_{yrated} + b_{yreleased}b_{yrated} + \varepsilon_{u,i}$$

where $Y_{u,i}$ is the $i$-th movie rating of user $u$, $\mu$ is the overall mean, $b_i$ is the movie effect, $b_u$ is the user effect, $b_{yreleased}$ is year of movie release, $b_{yrated}$ is year of movie rated, $b_{yreleased}b_{yrated}$ and $\varepsilon_{u,i}$ is the error term. 

The tables below show the average rating of the top 10 frequent and 10 non-frequent interaction term. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
##############################################################
## Model 6: Modelling Evidence of Interaction Time Effects
##############################################################

# Interaction Effect: How movies released in year X was rated in year Y ? 
#Create time interaction effect column in train and test sets
train$cross_time <- paste(train$year_released, train$year_rated, sep = "_") # Create time Interaction effect column in train set
test$cross_time <- paste(test$year_released, test$year_rated, sep = "_") # Create time Interaction effect column in test set


##Interaction Effect: Year Released and Year Rated
#Obtain Summary of Ratings by Year of Movie Release (Sum and Frequency of Ratings)
ratings_inteffect <- train %>% group_by(cross_time) %>% 
  mutate(Sum = sum(rating), Frequency = n()) %>% 
  select("cross_time", "Frequency", "Sum") %>%
  distinct()

#Obtain Average ratings to 4 decimal places
ratings_inteffect <- as.data.frame(ratings_inteffect) #Convert tibble to dataframe to easily obtain more than 2 decimals in manipulation
ratings_inteffect$Average_rating <- round((ratings_inteffect$Sum/ratings_inteffect$Frequency), 4) #Obtain Average ratings to 4 decimal places
ratings_inteffect$Sum <- NULL  #Remove redundant Sum column
ratings_inteffect <- arrange(ratings_inteffect, desc(Average_rating)) # Arrange dataframe in descending order of Average Ratings
ratings_inteffect %>% 
  arrange(desc(Frequency), Average_rating) %>% 
  head(.,10) %>%
  knitr::kable(., col.names = c("Year Released Year Rated", "Frequency", "Average Rating")) #Rating for the top 10 with highest frequency

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
ratings_inteffect %>% 
  arrange(Frequency, Average_rating) %>% 
  head(.,10) %>% 
  knitr::kable(., col.names = c("Year Released Year Rated", "Frequency", "Average Rating")) #Rating for the top 10 with lowest frequency
```

The top 10 most frequent combination of year released/year rated movies have ratings between 3.41 and 3.57. However, for the non-frequent interaction, the average rating ranges from 1.0 to 5.0, though the modal rating is 3.0. Due to the inclusion of the interaction term, there could be the possibility of predictions that could result in missing values or outside the actual rating bound of 0.5 and 5.0. Specifically, only one prediction came out as missing value and was imputed by the average value of predicted ratings which has been shown to be similar with both the top 10 frequent and 10 non-frequent interaction terms shown in the tables above. Also, predicted ratings above the rating ceiling of 5.0 were set to the ceiling whiles the predicated ratings below the rating floor of 0.5 were set to the rating floor.

```{r message=FALSE, warning=FALSE, include=FALSE}
#Incorporate interaction effect
inttime_effect <- train %>%
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(user_avgs, by = "userId") %>% 
  left_join(yreleased_effect, by = "year_released") %>%
  left_join(yrated_effect, by = "year_rated") %>%
  group_by(cross_time) %>%
  summarise(b_inttime = mean(rating - mu - b_i - b_u - b_yreleased - b_yrated))

  
# Estimate the predicted ratings on test set
predicted_ratings <- test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(yreleased_effect, by = 'year_released') %>%
  left_join(yrated_effect, by='year_rated') %>%
  left_join(inttime_effect, by = "cross_time") %>%
  mutate(pred = mu + b_i + b_u + b_yreleased + b_yrated + b_inttime) %>%
  .$pred

summary(predicted_ratings) #Examine Summary statistics of predicted ratings

predicted_ratings <- ifelse(is.na(predicted_ratings), mean(predicted_ratings, na.rm = TRUE), predicted_ratings) #replace the NA with mean value
predicted_ratings <- ifelse(predicted_ratings < 0.5, 0.5, predicted_ratings) # Ensure Rating Floor of 0.5
predicted_ratings <- ifelse(predicted_ratings > 5, 5, predicted_ratings) # Ensure Rating Ceiling of 5

# Obtain RMSE for model 
rmse_model6 <- RMSE(test$rating, predicted_ratings)
rmse_model6
```

The RMSE of the resulting model at this stage on the test set is `r round(rmse_model6, 6)`. 


# 3. Results

In the results section, we re-examine the modeling results and discuss the model performance. The final model resulted in the lowest RMSE on the test and would be evaluated on the validation set. 


```{r message=FALSE, warning=FALSE, include=FALSE}
##############################################################
## Validation Set : Examining Model 6 on validation set
##############################################################

###Generate required year effect columns
validation$year_rated <- year(as_datetime(validation$timestamp))   # Create year movie was rated column
validation$year_released <- gsub(".*[(]|[)].*", "", validation$title)   # Create year movie was released column
validation$cross_time <- paste(validation$year_released, validation$year_rated, sep = "_") # Create time Interaction effect column

# Estimate the predicted ratings on validation set
predicted_val <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(yreleased_effect, by = 'year_released') %>%
  left_join(yrated_effect, by='year_rated') %>%
  left_join(inttime_effect, by = "cross_time") %>%
  mutate(pred = mu + b_i + b_u + b_yreleased + b_yrated + b_inttime) %>%
  .$pred


summary(predicted_val) #Examine Summary statistics of predicted ratings

predicted_val <- ifelse(is.na(predicted_val), mean(predicted_val, na.rm = TRUE), predicted_val) #replace the NA with mean value
predicted_val <- ifelse(predicted_val < 0.5, 0.5, predicted_val) # Ensure Rating Floor of 0.5
predicted_val <- ifelse(predicted_val > 5, 5, predicted_val) # Ensure Rating Ceiling of 5


# Obtain RMSE for model 
rmse_val <- RMSE(validation$rating, predicted_val)
rmse_val

```

The results of all the methods are presented below. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
##Create dataframe of Performance Metrics of the Models
Method <- c('Just the Average', 'Movie Effect Model on Test Set', 'Movie and User Effects Model on Test Set', 
            'Reg. Movie and User Effects Model on Test Set', 'Movie, User, Year Released and Year Rated Effects Model on Test Set',
            'Movie, User, Year Released, Year Rated and Year Interaction Effects Model on Test Set',
            'Movie, User, Year Released, Year Rated and Year Interaction Effects Model on Validation Set') #Create row names

RMSE <- c(round(rmse_model1, 6), round(rmse_model2, 6), round(rmse_model3, 6), round(rmse_model4, 6), round(rmse_model5, 6), 
          round(rmse_model6, 6), round(rmse_val, 6)) # Place model RMSE results into corresponding vector


model_results <- data.frame(Method, RMSE) # Create dataframe of the model results
#print out the results
model_results %>% knitr::kable()
```


As noted earlier, preferred models should have an RMSE below 0.86490 on the validation set. From the table above, we note that the lowest RMSE on the test set was `r round(rmse_model6, 6)`. Thus, the chosen model is the final model, the model that involved the inclusion of the interaction term. 
Moreover, the chosen model performed very well on the validation set, producing an RMSE of **`r round(rmse_val, 6)`**, similar to the RMSE on the test data and less than the threshold of 0.86490. Therefore, the result is robust to either over-fitting or under-fitting. It is expected that the model will still perform well on another set of new data. 


# 4. Conclusion

In this report, we examined model techniques in predicting movie ratings using RMSE as the metric of evaluation. The data used to train the model comprised `r summary_train$n_users` unique users rating `r summary_train$n_movies` unique movies, an indication that number of movie ratings were unequal among users. However, regularization technique shown that accounting for the unequal numbers did not account for significant bias in training. Overall, the chosen model involved the inclusion of interaction time effects of year of movie release and year of movie rated, after accounting for user, movie, year of movie release and year of movie rated. The model produced an RMSE of `r round(rmse_val, 6)` on the validation set, lower than the threshold of 0.86490. 

That notwithstanding, we acknowledge some limitations. First, more advanced algorithms such as matrix factorization could produce superior RMSE. However, current matrix factorization could be computationally expensive and time consuming. Also, movie ratings can be thought of as a form of utility maximization, where ratings are used to express satisfaction. In this, utility studies suggest that taste changes, making it possible for the same users to rate the same movie differently under different circumstances. In effect, the data used in the estimation was obtained at a point in time in the past which may not be consistent over time. Again, the data does not tell us how the ratings were obtained. This is important because it is possible for non-humans (computers) to provide ratings, borne out of marketing techniques. Such artificial ratings if prevalent could undermine the modelling of human user ratings. 

Recommendation systems tend to be important component of building successful businesses nowadays. Future work could expand on computationally inexpensive but advanced algorithms and economic motivations of users in their rating decisions. Additionally, future research could also incorporate indications of general locality to determine whether there are geographical effects. In all analysis, it is recommended that obvious individual identifier information are limited to the barest minimum. 



#### Reference
Irizarry, R.A. (2022).Introduction to Data Science: Data Analysis and Prediction Algorithms with R. *The coding in this report was built on primary code provided by the course as contained in Irizarry(2022)*.  

